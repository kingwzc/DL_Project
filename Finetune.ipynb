{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Finetune.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a477fa34922848b58588072690765a72":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_db760264293841f291995898ea5da41f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cef9a38f052245198505e5ec97aad195","IPY_MODEL_f52bcc54dac94236b8b8d6ae9efaf8c1"]}},"db760264293841f291995898ea5da41f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cef9a38f052245198505e5ec97aad195":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_685476c057b7461295afa9f01acd219a","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":102502400,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":102502400,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f72eb01b752144cfb4bb92bc49e0bd99"}},"f52bcc54dac94236b8b8d6ae9efaf8c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1e2f354935c443c09faef6c585c1e6a3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 97.8M/97.8M [00:01&lt;00:00, 98.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ef5ada90ba5e4f3f9e24eb7a9ae63bc4"}},"685476c057b7461295afa9f01acd219a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f72eb01b752144cfb4bb92bc49e0bd99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1e2f354935c443c09faef6c585c1e6a3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ef5ada90ba5e4f3f9e24eb7a9ae63bc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"UWj9eJGjO22g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621240817987,"user_tz":-480,"elapsed":691,"user":{"displayName":"Yujun Kong","photoUrl":"","userId":"16664495913434421972"}},"outputId":"e3d100c5-55b6-4e8a-c2ed-7e1f0cb702d7"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir('/content/drive/My Drive/DL/Final_submission')\n","import torch.optim as optim\n","#from torch.optim import lr_scheduler\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib\n","import matplotlib.pyplot as plt\n","matplotlib.rcParams['figure.figsize'] = [5, 5]\n","matplotlib.rcParams['figure.dpi'] = 200\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","from data_helper import UnlabeledDataset, LabeledDataset\n","from helper import collate_fn, draw_box\n","from torchvision import models\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","from torchvision.models.detection.backbone_utils import BackboneWithFPN\n","from torchvision.models import resnet\n","from torchvision.models._utils import IntermediateLayerGetter\n","from torchvision.ops import misc as misc_nn_ops\n","\n","cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n","\n","\n","random.seed(0)\n","np.random.seed(0)\n","torch.manual_seed(0)\n","\n","# All the images are saved in image_folder\n","# All the labels are saved in the annotation_csv file\n","image_folder = 'data'\n","annotation_csv = 'data/annotation.csv'\n","# Set up your device \n","cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n","print(device)\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3XU9qAbw3ccP","executionInfo":{"status":"ok","timestamp":1621240820946,"user_tz":-480,"elapsed":1157,"user":{"displayName":"Yujun Kong","photoUrl":"","userId":"16664495913434421972"}}},"source":["#backbone class from pirl jigsaw\n","class Representation_Generator(nn.Module):\n","    \"\"\"Class that returns features for original image and fused features for image patches;\n","       its backbone is what we used for downstream task. \"\"\"\n","\n","    def __init__(self):\n","        super(Representation_Generator, self).__init__()\n","        #self.backbone = torch.nn.Sequential(*list(resnet50().children())[:-2]) \n","        self.backbone = torchvision.models.segmentation.fcn_resnet50(pretrained=False).backbone\n","        #Should we add pyramid network in it as in faster_rcnn ???\n","        self.pool = nn.AdaptiveAvgPool2d((1,1)) #pool the spatial dimension to be 1*1\n","        self.head_f = nn.Linear(2048, 128) \n","        self.head_g = nn.Linear(9*2048,128)\n","\n","    def forward(self, images, patches = None):\n","        image_feat = self.pool(self.backbone(images)['out'])\n","        image_feat = image_feat.view(-1,2048) #batch size, 2048\n","        image_feat = self.head_f(image_feat) #batch size, 128\n","\n","        if patches is not None:\n","            patches_feat = []\n","            for i, patch in enumerate(patches):\n","                \n","                patch_feat = self.pool(self.backbone(patch)['out']) #batch size, 2048, 1,1\n","                patch_feat = patch_feat.view(-1,2048) # batch_size,2048\n","                patches_feat.append(patch_feat)\n","         \n","            patches_feat = torch.cat(patches_feat, axis = 1) #batch size, 2048*9, \n","            patches_feat = self.head_g(patches_feat)   #batch size, 128  \n","\n","            return image_feat, patches_feat\n","        else:\n","            return image_feat\n","\n","\n","#phrase pirl backbone for object detection: add FPN and freeze BN\n","def resnet_fpn_backbone(backbone, freeze_bn = True):\n","\n","    # copied the behaviour from faster_rcnn, freeze layer1\n","    for name, parameter in backbone.named_parameters():\n","        if 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n","            parameter.requires_grad_(False)\n","\n","    return_layers = {'layer1': '0', 'layer2': '1', 'layer3': '2', 'layer4': '3'}\n","\n","    in_channels_stage2 = 2048 // 8 #64 is resnet's inplanes\n","    in_channels_list = [\n","        in_channels_stage2,\n","        in_channels_stage2 * 2,\n","        in_channels_stage2 * 4,\n","        in_channels_stage2 * 8,\n","    ]\n","    out_channels = 256\n","    bb_fpn = BackboneWithFPN(backbone, return_layers, in_channels_list, out_channels)\n","   \n","    def set_bn_eval(m): \n","        \n","        #freeze the batchnorm2d layer: in object detection we are using small batch size, so we don't want to track batch statistics cause they are poor\n","        classname = m.__class__.__name__\n","        if classname.find('BatchNorm2d') != -1:\n","            m.eval()\n","    \n","    if freeze_bn:\n","        return bb_fpn.apply(set_bn_eval)\n","    else:\n","        return bb_fpn\n","\n","\n","#detection_backbone = resnet_fpn_backbone(torchvision.models.segmentation.fcn_resnet50(pretrained=False).backbone)\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZwspD4qZZGaE","executionInfo":{"status":"ok","timestamp":1621240823411,"user_tz":-480,"elapsed":1323,"user":{"displayName":"Yujun Kong","photoUrl":"","userId":"16664495913434421972"}}},"source":["# model classes\n","\n","# road segmentation's classifier\n","class FCNHead(nn.Sequential):\n","    def __init__(self, in_channels, num_classes):\n","        \"\"\"in_channels: dim of input feature map after fusion\n","           \"\"\"\n","        self.inter_channels = in_channels // 4\n","        self.layers = [\n","            nn.Conv2d(in_channels, self.inter_channels, 3, padding=1, bias=False),\n","            nn.BatchNorm2d(self.inter_channels),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Conv2d(self.inter_channels, num_classes, 1)\n","        ]\n","\n","        super(FCNHead, self).__init__(*self.layers)\n","\n","#road segmentation's model\n","class Road_Layout(nn.Module):\n","  def __init__(self, backbone, classifier):\n","        super(Road_Layout, self).__init__()\n","        #images, targets = self.transform(images, targets)\n","        self.backbone = backbone\n","        self.classifier = classifier\n"," \n","\n","  def forward(self, images, targets=None):\n","        feats= []\n","        for view in range(6):\n","            feats.append(self.backbone(images[:,view,:])[\"out\"])\n","            #concatenate feature map from all views\n","        #fused_feature = torch.cat(feats,dim = 1) #(batch_size, 6*fused channels, H, W )\n","        fused_feature = torch.mean(torch.stack(feats),dim = 0)\n","        x = self.classifier(fused_feature) #(batch size, num_classes, H,W)\n","        x = F.interpolate(x, size=(800,800), mode='bilinear', align_corners=False)\n","\n","        return x #(batch_size, num_classes, 800,800)\n","\n","\n","# object detection  - generate top_down layer\n","class Fusion_Layer(nn.Module):\n","    \"\"\"Model to generate  800 * 800 size road map / Convert to Bird Eye View;\n","       road_model_feat is the feature map output from the road_model, assumed to have (h,w) the same as backbone output feat dim;\n","       mean?? project with camera intrinsics??\"\"\"\n","    def __init__(self, backbone, feature_channels, road_model_feat = None):\n","        super(Fusion_Layer, self).__init__()    \n","        tot_channels = feature_channels \n","        self.road_model_feat = road_model_feat\n","        if road_model_feat is not None:\n","            tot_channels += road_model_feat.size()[1]\n","        \n","        #for mapping back to 800*800\n","        self.backbone = backbone\n","        self.relu =  nn.ReLU()\n","        self.deconv1 = nn.ConvTranspose2d(tot_channels, tot_channels, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn1 = nn.BatchNorm2d(tot_channels)\n","        self.deconv2 = nn.ConvTranspose2d(tot_channels, tot_channels//4, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn2 = nn.BatchNorm2d(tot_channels//4)\n","        self.deconv3 = nn.ConvTranspose2d(tot_channels//4, tot_channels//16, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn3 = nn.BatchNorm2d(tot_channels//16)\n","        self.deconv4 = nn.ConvTranspose2d(tot_channels//16, tot_channels//16, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn4 = nn.BatchNorm2d(tot_channels//16)\n","        self.deconv5 = nn.ConvTranspose2d(tot_channels//16, 3, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn5 = nn.BatchNorm2d(3)\n","\n","    def forward(self, images):\n","        feats= []\n","        for view in range(6):\n","            feats.append(self.backbone(images[:,view,:])[\"out\"])\n","        fused_feature = torch.mean(torch.stack(feats),dim = 0)\n","        # fused_feature = torch.cat(feats,dim = 1)\n","        if self.road_model_feat is not None:\n","            fused_feature = toch.cat([fused_feature,self.road_model_feat],dim = 1)\n","\n","        #transform into (batch_size, channels, 800,800)\n","        x1 = self.bn1(self.relu(self.deconv1(fused_feature))) # (H,W) -> 2*(H,W)\n","        x2 = self.bn2(self.relu(self.deconv2(x1)))\n","        x3 = self.bn3(self.relu(self.deconv3(x2)))\n","        x4 = self.bn4(self.relu(self.deconv4(x3)))\n","        x5 = self.bn5(self.relu(self.deconv5(x4)))\n","\n","        return x5\n","\n","#detection model\n","class Box_Model(nn.Module):\n","    def __init__(self, fuse_layer, detect_model):\n","        super(Box_Model, self).__init__()\n","        self.fuse_layer = fuse_layer\n","        self.detect_model  = detect_model\n","\n","    def transform_target(self,targets):\n","        res = [] #targets should be a list of dictionaries with key \"boxes\" and \"labels\"\n","        for t in targets:\n","            N = t['category'].size(0) #number of boxes\n","            bbox = torch.zeros(N,4)\n","            for n in range(N):\n","                nth_box = 10*t['bounding_box'][n] #shape (2 ,4) multiply 10 because the original value is in meters!\n","                xmax = torch.max(nth_box[0,:]) \n","                xmin = torch.min(nth_box[0,:])\n","                ymax = torch.max(nth_box[1,:]) \n","                ymin = torch.min(nth_box[1,:])\n","                bbox[n] = torch.tensor([xmin,ymin,xmax,ymax]) + 400\n","            res.append({\"boxes\":bbox.to(device),\"labels\":t['category'].to(device)})\n","        return res \n","\n","    def get_output(self,preds):\n","        pred_box = []\n","        pred_label = []\n","        for p in preds: \n","            nbox = p['boxes'].size(0)\n","            res_box = torch.zeros(nbox, 2, 4)\n","            res_label = torch.zeros(nbox)\n","            for n in range(nbox):\n","                xmin, ymin, xmax, ymax = p['boxes'][n] #in pixel level\n","                res_box[n] = torch.tensor([[xmax, xmax, xmin, xmin],[ymax,ymin,ymax, ymin]])\n","                res_label[n] = p['labels'][n]\n","            res_box = (res_box - 400)/10 #the unit should be meter instead of pixels\n","            pred_box.append(res_box)\n","            pred_label.append(res_label)\n","        return {\"boxes\":tuple(pred_box), \"labels\":tuple(pred_label)}\n","\n","\n","    def forward(self, images, targets  = None):\n","\n","        top_down = self.fuse_layer(images) #(batch_size, 3, 800, 800)\n","        top_down  = [i for i in top_down]\n","        if self.training:\n","            self.detect_model.train() \n","            targets = self.transform_target(targets)\n","            output = self.detect_model(top_down,targets) #loss_dict\n","        else:\n","            preds = self.detect_model(top_down)#list of dictionary of keys 'boxes', 'labels', 'scores'\n","            #need to transform predicted boxes coordinates, a torch tensor of size (num_boxes, 2, 4)\n","            output = self.get_output(preds)\n","\n","        return output #at eval mode, output is a dictionary: \"boxes\": a tuple of tensors of size (num_boxes, 2, 4), \"labels\": a tuple of tensor (num_boxes)\n","\n","\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"tsNd7Sh-bCZQ","executionInfo":{"status":"ok","timestamp":1621240827223,"user_tz":-480,"elapsed":1046,"user":{"displayName":"Yujun Kong","photoUrl":"","userId":"16664495913434421972"}}},"source":["#model loader\n","##get model to train: model_loader.road_model and model_loader.box_model\n","class ModelLoader():\n","    def __init__(self, model_file='models_final.py', test = False):\n","        # 1. create the model object\n","        # 2. load your state_dict\n","        # 3. call cuda()\n","        # road segmentation\n","\n","        rep_net1 = Representation_Generator()\n","        rep_net2 = Representation_Generator()\n","        rep_net1.load_state_dict(torch.load('./model/pirl_jigsaw/rep_net3.pth',map_location=torch.device('cpu')))\n","        rep_net2.load_state_dict(torch.load('./model/pirl_jigsaw/rep_net3.pth',map_location=torch.device('cpu')))\n","        print(\"Load pretrained backbone successfully!\")\n","        #allow reconstruction of topdown view and object detection to use separate backbone\n","        self.reconstruct_backbone = rep_net1.backbone \n","        self.detection_backbone = resnet_fpn_backbone(rep_net2.backbone) #make it conformed with faster_rcnn\n","\n","        #self.backbone = models.segmentation.fcn_resnet50(num_classes = 2, pretrained=False).backbone\n","        self.classifier = FCNHead(2048, 2)\n","        self.road_model = Road_Layout(self.reconstruct_backbone, self.classifier)\n","\n","        # object detection\n","        self.num_classes = 10\n","        self.anchor_generator = AnchorGenerator( ((8,),(16,),(32,), (64,), (128,)),\n","                                        ((0.5, 1.0, 2.0),) *5)\n","        self.roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], #cooresponds to 4 layers in resnet\n","                                                        output_size=7,\n","                                                        sampling_ratio=2)\n","\n","        ## put the pieces together inside a FasterRCNN model, input needs to be (batch_size, channels, H, W)\n","        self.detect_model = FasterRCNN(self.detection_backbone,\n","                        num_classes = self.num_classes,\n","                        rpn_anchor_generator = self.anchor_generator,\n","                        box_roi_pool = self.roi_pooler)\n","\n","        #self.reconstruct_bone = torchvision.models.segmentation.fcn_resnet50(pretrained=False).backbone.to(device)\n","        self.FL = Fusion_Layer(self.reconstruct_backbone, 2048, road_model_feat = None)\n","        self.box_model = Box_Model(self.FL, self.detect_model)\n","\n","        #load model state_dict\n","        if test:\n","            self.road_model.load_state_dict(torch.load('./model/road_seg_BH/epoch_16_avg_val_loss_0.04860_avg_ts_0.91775_lr_0.0001000000_2.pth',map_location=torch.device('cpu')))\n","            self.road_model.eval()\n","\n","            self.box_model.load_state_dict(torch.load('./model/box_detect/fintune_epoch_2_avg_ats_0.01862_lr_0.0001000000.pth',map_location=torch.device('cpu')))\n","            self.box_model.eval()\n","\n","    def get_bounding_boxes(self, samples):\n","        # samples is a cuda tensor with size [batch_size, 6, 3, 256, 306]\n","        # return a tuple with size 'batch_size' and each element is a cuda tensor [N, 2, 4]\n","        # where N is the number of object\n","        with torch.no_grad():\n","            output = self.box_model(samples)\n","            pred_boxes = output['boxes']\n","            #Future work: deal with outputs with 0 objects\n","            if pred_boxes[0].numel() == 0:\n","                return tuple(torch.zeros(len(samples),1,2,4))\n","        return pred_boxes\n","\n","    def get_binary_road_map(self, samples):\n","        # samples is a cuda tensor with size [batch_size, 6, 3, 256, 306]\n","        # return a cuda tensor with size [batch_size, 800, 800]\n","        with torch.no_grad():\n","            output = self.road_model(samples)\n","\n","        return torch.argmax(output, dim=1)\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"-e38FfarjRiy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621240830273,"user_tz":-480,"elapsed":1654,"user":{"displayName":"Yujun Kong","photoUrl":"","userId":"16664495913434421972"}},"outputId":"765f198c-d269-4ab8-f0f8-3afe27f571f9"},"source":["!ls ./model/pirl_jigsaw/"],"execution_count":7,"outputs":[{"output_type":"stream","text":["rep_net2.pth  rep_net3.pth\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GhME29GQRZlX","colab":{"base_uri":"https://localhost:8080/","height":100,"referenced_widgets":["a477fa34922848b58588072690765a72","db760264293841f291995898ea5da41f","cef9a38f052245198505e5ec97aad195","f52bcc54dac94236b8b8d6ae9efaf8c1","685476c057b7461295afa9f01acd219a","f72eb01b752144cfb4bb92bc49e0bd99","1e2f354935c443c09faef6c585c1e6a3","ef5ada90ba5e4f3f9e24eb7a9ae63bc4"]},"executionInfo":{"status":"ok","timestamp":1621240837394,"user_tz":-480,"elapsed":6447,"user":{"displayName":"Yujun Kong","photoUrl":"","userId":"16664495913434421972"}},"outputId":"6074c69d-560a-4fdb-c629-f5ff34bb1e33"},"source":["model_loader = ModelLoader()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a477fa34922848b58588072690765a72","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Load pretrained backbone successfully!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JKhKrumwmh8f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621240841307,"user_tz":-480,"elapsed":1468,"user":{"displayName":"Yujun Kong","photoUrl":"","userId":"16664495913434421972"}},"outputId":"ecc9f375-d0b2-4a8a-d478-28d9879b8ec6"},"source":["def num_of_trained_parameters(model):\n","    for x in model.named_parameters():\n","        print (x[0],x[1].size())\n","    #return sum(param.numel() for param in model.parameters() if param.requires_grad)\n","    return sum(param.numel() for param in model.parameters() if param.requires_grad)\n","num_of_trained_parameters(model_loader.box_model)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["fuse_layer.backbone.conv1.weight torch.Size([64, 3, 7, 7])\n","fuse_layer.backbone.bn1.weight torch.Size([64])\n","fuse_layer.backbone.bn1.bias torch.Size([64])\n","fuse_layer.backbone.layer1.0.conv1.weight torch.Size([64, 64, 1, 1])\n","fuse_layer.backbone.layer1.0.bn1.weight torch.Size([64])\n","fuse_layer.backbone.layer1.0.bn1.bias torch.Size([64])\n","fuse_layer.backbone.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n","fuse_layer.backbone.layer1.0.bn2.weight torch.Size([64])\n","fuse_layer.backbone.layer1.0.bn2.bias torch.Size([64])\n","fuse_layer.backbone.layer1.0.conv3.weight torch.Size([256, 64, 1, 1])\n","fuse_layer.backbone.layer1.0.bn3.weight torch.Size([256])\n","fuse_layer.backbone.layer1.0.bn3.bias torch.Size([256])\n","fuse_layer.backbone.layer1.0.downsample.0.weight torch.Size([256, 64, 1, 1])\n","fuse_layer.backbone.layer1.0.downsample.1.weight torch.Size([256])\n","fuse_layer.backbone.layer1.0.downsample.1.bias torch.Size([256])\n","fuse_layer.backbone.layer1.1.conv1.weight torch.Size([64, 256, 1, 1])\n","fuse_layer.backbone.layer1.1.bn1.weight torch.Size([64])\n","fuse_layer.backbone.layer1.1.bn1.bias torch.Size([64])\n","fuse_layer.backbone.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n","fuse_layer.backbone.layer1.1.bn2.weight torch.Size([64])\n","fuse_layer.backbone.layer1.1.bn2.bias torch.Size([64])\n","fuse_layer.backbone.layer1.1.conv3.weight torch.Size([256, 64, 1, 1])\n","fuse_layer.backbone.layer1.1.bn3.weight torch.Size([256])\n","fuse_layer.backbone.layer1.1.bn3.bias torch.Size([256])\n","fuse_layer.backbone.layer1.2.conv1.weight torch.Size([64, 256, 1, 1])\n","fuse_layer.backbone.layer1.2.bn1.weight torch.Size([64])\n","fuse_layer.backbone.layer1.2.bn1.bias torch.Size([64])\n","fuse_layer.backbone.layer1.2.conv2.weight torch.Size([64, 64, 3, 3])\n","fuse_layer.backbone.layer1.2.bn2.weight torch.Size([64])\n","fuse_layer.backbone.layer1.2.bn2.bias torch.Size([64])\n","fuse_layer.backbone.layer1.2.conv3.weight torch.Size([256, 64, 1, 1])\n","fuse_layer.backbone.layer1.2.bn3.weight torch.Size([256])\n","fuse_layer.backbone.layer1.2.bn3.bias torch.Size([256])\n","fuse_layer.backbone.layer2.0.conv1.weight torch.Size([128, 256, 1, 1])\n","fuse_layer.backbone.layer2.0.bn1.weight torch.Size([128])\n","fuse_layer.backbone.layer2.0.bn1.bias torch.Size([128])\n","fuse_layer.backbone.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n","fuse_layer.backbone.layer2.0.bn2.weight torch.Size([128])\n","fuse_layer.backbone.layer2.0.bn2.bias torch.Size([128])\n","fuse_layer.backbone.layer2.0.conv3.weight torch.Size([512, 128, 1, 1])\n","fuse_layer.backbone.layer2.0.bn3.weight torch.Size([512])\n","fuse_layer.backbone.layer2.0.bn3.bias torch.Size([512])\n","fuse_layer.backbone.layer2.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n","fuse_layer.backbone.layer2.0.downsample.1.weight torch.Size([512])\n","fuse_layer.backbone.layer2.0.downsample.1.bias torch.Size([512])\n","fuse_layer.backbone.layer2.1.conv1.weight torch.Size([128, 512, 1, 1])\n","fuse_layer.backbone.layer2.1.bn1.weight torch.Size([128])\n","fuse_layer.backbone.layer2.1.bn1.bias torch.Size([128])\n","fuse_layer.backbone.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n","fuse_layer.backbone.layer2.1.bn2.weight torch.Size([128])\n","fuse_layer.backbone.layer2.1.bn2.bias torch.Size([128])\n","fuse_layer.backbone.layer2.1.conv3.weight torch.Size([512, 128, 1, 1])\n","fuse_layer.backbone.layer2.1.bn3.weight torch.Size([512])\n","fuse_layer.backbone.layer2.1.bn3.bias torch.Size([512])\n","fuse_layer.backbone.layer2.2.conv1.weight torch.Size([128, 512, 1, 1])\n","fuse_layer.backbone.layer2.2.bn1.weight torch.Size([128])\n","fuse_layer.backbone.layer2.2.bn1.bias torch.Size([128])\n","fuse_layer.backbone.layer2.2.conv2.weight torch.Size([128, 128, 3, 3])\n","fuse_layer.backbone.layer2.2.bn2.weight torch.Size([128])\n","fuse_layer.backbone.layer2.2.bn2.bias torch.Size([128])\n","fuse_layer.backbone.layer2.2.conv3.weight torch.Size([512, 128, 1, 1])\n","fuse_layer.backbone.layer2.2.bn3.weight torch.Size([512])\n","fuse_layer.backbone.layer2.2.bn3.bias torch.Size([512])\n","fuse_layer.backbone.layer2.3.conv1.weight torch.Size([128, 512, 1, 1])\n","fuse_layer.backbone.layer2.3.bn1.weight torch.Size([128])\n","fuse_layer.backbone.layer2.3.bn1.bias torch.Size([128])\n","fuse_layer.backbone.layer2.3.conv2.weight torch.Size([128, 128, 3, 3])\n","fuse_layer.backbone.layer2.3.bn2.weight torch.Size([128])\n","fuse_layer.backbone.layer2.3.bn2.bias torch.Size([128])\n","fuse_layer.backbone.layer2.3.conv3.weight torch.Size([512, 128, 1, 1])\n","fuse_layer.backbone.layer2.3.bn3.weight torch.Size([512])\n","fuse_layer.backbone.layer2.3.bn3.bias torch.Size([512])\n","fuse_layer.backbone.layer3.0.conv1.weight torch.Size([256, 512, 1, 1])\n","fuse_layer.backbone.layer3.0.bn1.weight torch.Size([256])\n","fuse_layer.backbone.layer3.0.bn1.bias torch.Size([256])\n","fuse_layer.backbone.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n","fuse_layer.backbone.layer3.0.bn2.weight torch.Size([256])\n","fuse_layer.backbone.layer3.0.bn2.bias torch.Size([256])\n","fuse_layer.backbone.layer3.0.conv3.weight torch.Size([1024, 256, 1, 1])\n","fuse_layer.backbone.layer3.0.bn3.weight torch.Size([1024])\n","fuse_layer.backbone.layer3.0.bn3.bias torch.Size([1024])\n","fuse_layer.backbone.layer3.0.downsample.0.weight torch.Size([1024, 512, 1, 1])\n","fuse_layer.backbone.layer3.0.downsample.1.weight torch.Size([1024])\n","fuse_layer.backbone.layer3.0.downsample.1.bias torch.Size([1024])\n","fuse_layer.backbone.layer3.1.conv1.weight torch.Size([256, 1024, 1, 1])\n","fuse_layer.backbone.layer3.1.bn1.weight torch.Size([256])\n","fuse_layer.backbone.layer3.1.bn1.bias torch.Size([256])\n","fuse_layer.backbone.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n","fuse_layer.backbone.layer3.1.bn2.weight torch.Size([256])\n","fuse_layer.backbone.layer3.1.bn2.bias torch.Size([256])\n","fuse_layer.backbone.layer3.1.conv3.weight torch.Size([1024, 256, 1, 1])\n","fuse_layer.backbone.layer3.1.bn3.weight torch.Size([1024])\n","fuse_layer.backbone.layer3.1.bn3.bias torch.Size([1024])\n","fuse_layer.backbone.layer3.2.conv1.weight torch.Size([256, 1024, 1, 1])\n","fuse_layer.backbone.layer3.2.bn1.weight torch.Size([256])\n","fuse_layer.backbone.layer3.2.bn1.bias torch.Size([256])\n","fuse_layer.backbone.layer3.2.conv2.weight torch.Size([256, 256, 3, 3])\n","fuse_layer.backbone.layer3.2.bn2.weight torch.Size([256])\n","fuse_layer.backbone.layer3.2.bn2.bias torch.Size([256])\n","fuse_layer.backbone.layer3.2.conv3.weight torch.Size([1024, 256, 1, 1])\n","fuse_layer.backbone.layer3.2.bn3.weight torch.Size([1024])\n","fuse_layer.backbone.layer3.2.bn3.bias torch.Size([1024])\n","fuse_layer.backbone.layer3.3.conv1.weight torch.Size([256, 1024, 1, 1])\n","fuse_layer.backbone.layer3.3.bn1.weight torch.Size([256])\n","fuse_layer.backbone.layer3.3.bn1.bias torch.Size([256])\n","fuse_layer.backbone.layer3.3.conv2.weight torch.Size([256, 256, 3, 3])\n","fuse_layer.backbone.layer3.3.bn2.weight torch.Size([256])\n","fuse_layer.backbone.layer3.3.bn2.bias torch.Size([256])\n","fuse_layer.backbone.layer3.3.conv3.weight torch.Size([1024, 256, 1, 1])\n","fuse_layer.backbone.layer3.3.bn3.weight torch.Size([1024])\n","fuse_layer.backbone.layer3.3.bn3.bias torch.Size([1024])\n","fuse_layer.backbone.layer3.4.conv1.weight torch.Size([256, 1024, 1, 1])\n","fuse_layer.backbone.layer3.4.bn1.weight torch.Size([256])\n","fuse_layer.backbone.layer3.4.bn1.bias torch.Size([256])\n","fuse_layer.backbone.layer3.4.conv2.weight torch.Size([256, 256, 3, 3])\n","fuse_layer.backbone.layer3.4.bn2.weight torch.Size([256])\n","fuse_layer.backbone.layer3.4.bn2.bias torch.Size([256])\n","fuse_layer.backbone.layer3.4.conv3.weight torch.Size([1024, 256, 1, 1])\n","fuse_layer.backbone.layer3.4.bn3.weight torch.Size([1024])\n","fuse_layer.backbone.layer3.4.bn3.bias torch.Size([1024])\n","fuse_layer.backbone.layer3.5.conv1.weight torch.Size([256, 1024, 1, 1])\n","fuse_layer.backbone.layer3.5.bn1.weight torch.Size([256])\n","fuse_layer.backbone.layer3.5.bn1.bias torch.Size([256])\n","fuse_layer.backbone.layer3.5.conv2.weight torch.Size([256, 256, 3, 3])\n","fuse_layer.backbone.layer3.5.bn2.weight torch.Size([256])\n","fuse_layer.backbone.layer3.5.bn2.bias torch.Size([256])\n","fuse_layer.backbone.layer3.5.conv3.weight torch.Size([1024, 256, 1, 1])\n","fuse_layer.backbone.layer3.5.bn3.weight torch.Size([1024])\n","fuse_layer.backbone.layer3.5.bn3.bias torch.Size([1024])\n","fuse_layer.backbone.layer4.0.conv1.weight torch.Size([512, 1024, 1, 1])\n","fuse_layer.backbone.layer4.0.bn1.weight torch.Size([512])\n","fuse_layer.backbone.layer4.0.bn1.bias torch.Size([512])\n","fuse_layer.backbone.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n","fuse_layer.backbone.layer4.0.bn2.weight torch.Size([512])\n","fuse_layer.backbone.layer4.0.bn2.bias torch.Size([512])\n","fuse_layer.backbone.layer4.0.conv3.weight torch.Size([2048, 512, 1, 1])\n","fuse_layer.backbone.layer4.0.bn3.weight torch.Size([2048])\n","fuse_layer.backbone.layer4.0.bn3.bias torch.Size([2048])\n","fuse_layer.backbone.layer4.0.downsample.0.weight torch.Size([2048, 1024, 1, 1])\n","fuse_layer.backbone.layer4.0.downsample.1.weight torch.Size([2048])\n","fuse_layer.backbone.layer4.0.downsample.1.bias torch.Size([2048])\n","fuse_layer.backbone.layer4.1.conv1.weight torch.Size([512, 2048, 1, 1])\n","fuse_layer.backbone.layer4.1.bn1.weight torch.Size([512])\n","fuse_layer.backbone.layer4.1.bn1.bias torch.Size([512])\n","fuse_layer.backbone.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n","fuse_layer.backbone.layer4.1.bn2.weight torch.Size([512])\n","fuse_layer.backbone.layer4.1.bn2.bias torch.Size([512])\n","fuse_layer.backbone.layer4.1.conv3.weight torch.Size([2048, 512, 1, 1])\n","fuse_layer.backbone.layer4.1.bn3.weight torch.Size([2048])\n","fuse_layer.backbone.layer4.1.bn3.bias torch.Size([2048])\n","fuse_layer.backbone.layer4.2.conv1.weight torch.Size([512, 2048, 1, 1])\n","fuse_layer.backbone.layer4.2.bn1.weight torch.Size([512])\n","fuse_layer.backbone.layer4.2.bn1.bias torch.Size([512])\n","fuse_layer.backbone.layer4.2.conv2.weight torch.Size([512, 512, 3, 3])\n","fuse_layer.backbone.layer4.2.bn2.weight torch.Size([512])\n","fuse_layer.backbone.layer4.2.bn2.bias torch.Size([512])\n","fuse_layer.backbone.layer4.2.conv3.weight torch.Size([2048, 512, 1, 1])\n","fuse_layer.backbone.layer4.2.bn3.weight torch.Size([2048])\n","fuse_layer.backbone.layer4.2.bn3.bias torch.Size([2048])\n","fuse_layer.deconv1.weight torch.Size([2048, 2048, 3, 3])\n","fuse_layer.deconv1.bias torch.Size([2048])\n","fuse_layer.bn1.weight torch.Size([2048])\n","fuse_layer.bn1.bias torch.Size([2048])\n","fuse_layer.deconv2.weight torch.Size([2048, 512, 3, 3])\n","fuse_layer.deconv2.bias torch.Size([512])\n","fuse_layer.bn2.weight torch.Size([512])\n","fuse_layer.bn2.bias torch.Size([512])\n","fuse_layer.deconv3.weight torch.Size([512, 128, 3, 3])\n","fuse_layer.deconv3.bias torch.Size([128])\n","fuse_layer.bn3.weight torch.Size([128])\n","fuse_layer.bn3.bias torch.Size([128])\n","fuse_layer.deconv4.weight torch.Size([128, 128, 3, 3])\n","fuse_layer.deconv4.bias torch.Size([128])\n","fuse_layer.bn4.weight torch.Size([128])\n","fuse_layer.bn4.bias torch.Size([128])\n","fuse_layer.deconv5.weight torch.Size([128, 3, 3, 3])\n","fuse_layer.deconv5.bias torch.Size([3])\n","fuse_layer.bn5.weight torch.Size([3])\n","fuse_layer.bn5.bias torch.Size([3])\n","detect_model.backbone.body.conv1.weight torch.Size([64, 3, 7, 7])\n","detect_model.backbone.body.bn1.weight torch.Size([64])\n","detect_model.backbone.body.bn1.bias torch.Size([64])\n","detect_model.backbone.body.layer1.0.conv1.weight torch.Size([64, 64, 1, 1])\n","detect_model.backbone.body.layer1.0.bn1.weight torch.Size([64])\n","detect_model.backbone.body.layer1.0.bn1.bias torch.Size([64])\n","detect_model.backbone.body.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n","detect_model.backbone.body.layer1.0.bn2.weight torch.Size([64])\n","detect_model.backbone.body.layer1.0.bn2.bias torch.Size([64])\n","detect_model.backbone.body.layer1.0.conv3.weight torch.Size([256, 64, 1, 1])\n","detect_model.backbone.body.layer1.0.bn3.weight torch.Size([256])\n","detect_model.backbone.body.layer1.0.bn3.bias torch.Size([256])\n","detect_model.backbone.body.layer1.0.downsample.0.weight torch.Size([256, 64, 1, 1])\n","detect_model.backbone.body.layer1.0.downsample.1.weight torch.Size([256])\n","detect_model.backbone.body.layer1.0.downsample.1.bias torch.Size([256])\n","detect_model.backbone.body.layer1.1.conv1.weight torch.Size([64, 256, 1, 1])\n","detect_model.backbone.body.layer1.1.bn1.weight torch.Size([64])\n","detect_model.backbone.body.layer1.1.bn1.bias torch.Size([64])\n","detect_model.backbone.body.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n","detect_model.backbone.body.layer1.1.bn2.weight torch.Size([64])\n","detect_model.backbone.body.layer1.1.bn2.bias torch.Size([64])\n","detect_model.backbone.body.layer1.1.conv3.weight torch.Size([256, 64, 1, 1])\n","detect_model.backbone.body.layer1.1.bn3.weight torch.Size([256])\n","detect_model.backbone.body.layer1.1.bn3.bias torch.Size([256])\n","detect_model.backbone.body.layer1.2.conv1.weight torch.Size([64, 256, 1, 1])\n","detect_model.backbone.body.layer1.2.bn1.weight torch.Size([64])\n","detect_model.backbone.body.layer1.2.bn1.bias torch.Size([64])\n","detect_model.backbone.body.layer1.2.conv2.weight torch.Size([64, 64, 3, 3])\n","detect_model.backbone.body.layer1.2.bn2.weight torch.Size([64])\n","detect_model.backbone.body.layer1.2.bn2.bias torch.Size([64])\n","detect_model.backbone.body.layer1.2.conv3.weight torch.Size([256, 64, 1, 1])\n","detect_model.backbone.body.layer1.2.bn3.weight torch.Size([256])\n","detect_model.backbone.body.layer1.2.bn3.bias torch.Size([256])\n","detect_model.backbone.body.layer2.0.conv1.weight torch.Size([128, 256, 1, 1])\n","detect_model.backbone.body.layer2.0.bn1.weight torch.Size([128])\n","detect_model.backbone.body.layer2.0.bn1.bias torch.Size([128])\n","detect_model.backbone.body.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n","detect_model.backbone.body.layer2.0.bn2.weight torch.Size([128])\n","detect_model.backbone.body.layer2.0.bn2.bias torch.Size([128])\n","detect_model.backbone.body.layer2.0.conv3.weight torch.Size([512, 128, 1, 1])\n","detect_model.backbone.body.layer2.0.bn3.weight torch.Size([512])\n","detect_model.backbone.body.layer2.0.bn3.bias torch.Size([512])\n","detect_model.backbone.body.layer2.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n","detect_model.backbone.body.layer2.0.downsample.1.weight torch.Size([512])\n","detect_model.backbone.body.layer2.0.downsample.1.bias torch.Size([512])\n","detect_model.backbone.body.layer2.1.conv1.weight torch.Size([128, 512, 1, 1])\n","detect_model.backbone.body.layer2.1.bn1.weight torch.Size([128])\n","detect_model.backbone.body.layer2.1.bn1.bias torch.Size([128])\n","detect_model.backbone.body.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n","detect_model.backbone.body.layer2.1.bn2.weight torch.Size([128])\n","detect_model.backbone.body.layer2.1.bn2.bias torch.Size([128])\n","detect_model.backbone.body.layer2.1.conv3.weight torch.Size([512, 128, 1, 1])\n","detect_model.backbone.body.layer2.1.bn3.weight torch.Size([512])\n","detect_model.backbone.body.layer2.1.bn3.bias torch.Size([512])\n","detect_model.backbone.body.layer2.2.conv1.weight torch.Size([128, 512, 1, 1])\n","detect_model.backbone.body.layer2.2.bn1.weight torch.Size([128])\n","detect_model.backbone.body.layer2.2.bn1.bias torch.Size([128])\n","detect_model.backbone.body.layer2.2.conv2.weight torch.Size([128, 128, 3, 3])\n","detect_model.backbone.body.layer2.2.bn2.weight torch.Size([128])\n","detect_model.backbone.body.layer2.2.bn2.bias torch.Size([128])\n","detect_model.backbone.body.layer2.2.conv3.weight torch.Size([512, 128, 1, 1])\n","detect_model.backbone.body.layer2.2.bn3.weight torch.Size([512])\n","detect_model.backbone.body.layer2.2.bn3.bias torch.Size([512])\n","detect_model.backbone.body.layer2.3.conv1.weight torch.Size([128, 512, 1, 1])\n","detect_model.backbone.body.layer2.3.bn1.weight torch.Size([128])\n","detect_model.backbone.body.layer2.3.bn1.bias torch.Size([128])\n","detect_model.backbone.body.layer2.3.conv2.weight torch.Size([128, 128, 3, 3])\n","detect_model.backbone.body.layer2.3.bn2.weight torch.Size([128])\n","detect_model.backbone.body.layer2.3.bn2.bias torch.Size([128])\n","detect_model.backbone.body.layer2.3.conv3.weight torch.Size([512, 128, 1, 1])\n","detect_model.backbone.body.layer2.3.bn3.weight torch.Size([512])\n","detect_model.backbone.body.layer2.3.bn3.bias torch.Size([512])\n","detect_model.backbone.body.layer3.0.conv1.weight torch.Size([256, 512, 1, 1])\n","detect_model.backbone.body.layer3.0.bn1.weight torch.Size([256])\n","detect_model.backbone.body.layer3.0.bn1.bias torch.Size([256])\n","detect_model.backbone.body.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n","detect_model.backbone.body.layer3.0.bn2.weight torch.Size([256])\n","detect_model.backbone.body.layer3.0.bn2.bias torch.Size([256])\n","detect_model.backbone.body.layer3.0.conv3.weight torch.Size([1024, 256, 1, 1])\n","detect_model.backbone.body.layer3.0.bn3.weight torch.Size([1024])\n","detect_model.backbone.body.layer3.0.bn3.bias torch.Size([1024])\n","detect_model.backbone.body.layer3.0.downsample.0.weight torch.Size([1024, 512, 1, 1])\n","detect_model.backbone.body.layer3.0.downsample.1.weight torch.Size([1024])\n","detect_model.backbone.body.layer3.0.downsample.1.bias torch.Size([1024])\n","detect_model.backbone.body.layer3.1.conv1.weight torch.Size([256, 1024, 1, 1])\n","detect_model.backbone.body.layer3.1.bn1.weight torch.Size([256])\n","detect_model.backbone.body.layer3.1.bn1.bias torch.Size([256])\n","detect_model.backbone.body.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n","detect_model.backbone.body.layer3.1.bn2.weight torch.Size([256])\n","detect_model.backbone.body.layer3.1.bn2.bias torch.Size([256])\n","detect_model.backbone.body.layer3.1.conv3.weight torch.Size([1024, 256, 1, 1])\n","detect_model.backbone.body.layer3.1.bn3.weight torch.Size([1024])\n","detect_model.backbone.body.layer3.1.bn3.bias torch.Size([1024])\n","detect_model.backbone.body.layer3.2.conv1.weight torch.Size([256, 1024, 1, 1])\n","detect_model.backbone.body.layer3.2.bn1.weight torch.Size([256])\n","detect_model.backbone.body.layer3.2.bn1.bias torch.Size([256])\n","detect_model.backbone.body.layer3.2.conv2.weight torch.Size([256, 256, 3, 3])\n","detect_model.backbone.body.layer3.2.bn2.weight torch.Size([256])\n","detect_model.backbone.body.layer3.2.bn2.bias torch.Size([256])\n","detect_model.backbone.body.layer3.2.conv3.weight torch.Size([1024, 256, 1, 1])\n","detect_model.backbone.body.layer3.2.bn3.weight torch.Size([1024])\n","detect_model.backbone.body.layer3.2.bn3.bias torch.Size([1024])\n","detect_model.backbone.body.layer3.3.conv1.weight torch.Size([256, 1024, 1, 1])\n","detect_model.backbone.body.layer3.3.bn1.weight torch.Size([256])\n","detect_model.backbone.body.layer3.3.bn1.bias torch.Size([256])\n","detect_model.backbone.body.layer3.3.conv2.weight torch.Size([256, 256, 3, 3])\n","detect_model.backbone.body.layer3.3.bn2.weight torch.Size([256])\n","detect_model.backbone.body.layer3.3.bn2.bias torch.Size([256])\n","detect_model.backbone.body.layer3.3.conv3.weight torch.Size([1024, 256, 1, 1])\n","detect_model.backbone.body.layer3.3.bn3.weight torch.Size([1024])\n","detect_model.backbone.body.layer3.3.bn3.bias torch.Size([1024])\n","detect_model.backbone.body.layer3.4.conv1.weight torch.Size([256, 1024, 1, 1])\n","detect_model.backbone.body.layer3.4.bn1.weight torch.Size([256])\n","detect_model.backbone.body.layer3.4.bn1.bias torch.Size([256])\n","detect_model.backbone.body.layer3.4.conv2.weight torch.Size([256, 256, 3, 3])\n","detect_model.backbone.body.layer3.4.bn2.weight torch.Size([256])\n","detect_model.backbone.body.layer3.4.bn2.bias torch.Size([256])\n","detect_model.backbone.body.layer3.4.conv3.weight torch.Size([1024, 256, 1, 1])\n","detect_model.backbone.body.layer3.4.bn3.weight torch.Size([1024])\n","detect_model.backbone.body.layer3.4.bn3.bias torch.Size([1024])\n","detect_model.backbone.body.layer3.5.conv1.weight torch.Size([256, 1024, 1, 1])\n","detect_model.backbone.body.layer3.5.bn1.weight torch.Size([256])\n","detect_model.backbone.body.layer3.5.bn1.bias torch.Size([256])\n","detect_model.backbone.body.layer3.5.conv2.weight torch.Size([256, 256, 3, 3])\n","detect_model.backbone.body.layer3.5.bn2.weight torch.Size([256])\n","detect_model.backbone.body.layer3.5.bn2.bias torch.Size([256])\n","detect_model.backbone.body.layer3.5.conv3.weight torch.Size([1024, 256, 1, 1])\n","detect_model.backbone.body.layer3.5.bn3.weight torch.Size([1024])\n","detect_model.backbone.body.layer3.5.bn3.bias torch.Size([1024])\n","detect_model.backbone.body.layer4.0.conv1.weight torch.Size([512, 1024, 1, 1])\n","detect_model.backbone.body.layer4.0.bn1.weight torch.Size([512])\n","detect_model.backbone.body.layer4.0.bn1.bias torch.Size([512])\n","detect_model.backbone.body.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n","detect_model.backbone.body.layer4.0.bn2.weight torch.Size([512])\n","detect_model.backbone.body.layer4.0.bn2.bias torch.Size([512])\n","detect_model.backbone.body.layer4.0.conv3.weight torch.Size([2048, 512, 1, 1])\n","detect_model.backbone.body.layer4.0.bn3.weight torch.Size([2048])\n","detect_model.backbone.body.layer4.0.bn3.bias torch.Size([2048])\n","detect_model.backbone.body.layer4.0.downsample.0.weight torch.Size([2048, 1024, 1, 1])\n","detect_model.backbone.body.layer4.0.downsample.1.weight torch.Size([2048])\n","detect_model.backbone.body.layer4.0.downsample.1.bias torch.Size([2048])\n","detect_model.backbone.body.layer4.1.conv1.weight torch.Size([512, 2048, 1, 1])\n","detect_model.backbone.body.layer4.1.bn1.weight torch.Size([512])\n","detect_model.backbone.body.layer4.1.bn1.bias torch.Size([512])\n","detect_model.backbone.body.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n","detect_model.backbone.body.layer4.1.bn2.weight torch.Size([512])\n","detect_model.backbone.body.layer4.1.bn2.bias torch.Size([512])\n","detect_model.backbone.body.layer4.1.conv3.weight torch.Size([2048, 512, 1, 1])\n","detect_model.backbone.body.layer4.1.bn3.weight torch.Size([2048])\n","detect_model.backbone.body.layer4.1.bn3.bias torch.Size([2048])\n","detect_model.backbone.body.layer4.2.conv1.weight torch.Size([512, 2048, 1, 1])\n","detect_model.backbone.body.layer4.2.bn1.weight torch.Size([512])\n","detect_model.backbone.body.layer4.2.bn1.bias torch.Size([512])\n","detect_model.backbone.body.layer4.2.conv2.weight torch.Size([512, 512, 3, 3])\n","detect_model.backbone.body.layer4.2.bn2.weight torch.Size([512])\n","detect_model.backbone.body.layer4.2.bn2.bias torch.Size([512])\n","detect_model.backbone.body.layer4.2.conv3.weight torch.Size([2048, 512, 1, 1])\n","detect_model.backbone.body.layer4.2.bn3.weight torch.Size([2048])\n","detect_model.backbone.body.layer4.2.bn3.bias torch.Size([2048])\n","detect_model.backbone.fpn.inner_blocks.0.weight torch.Size([256, 256, 1, 1])\n","detect_model.backbone.fpn.inner_blocks.0.bias torch.Size([256])\n","detect_model.backbone.fpn.inner_blocks.1.weight torch.Size([256, 512, 1, 1])\n","detect_model.backbone.fpn.inner_blocks.1.bias torch.Size([256])\n","detect_model.backbone.fpn.inner_blocks.2.weight torch.Size([256, 1024, 1, 1])\n","detect_model.backbone.fpn.inner_blocks.2.bias torch.Size([256])\n","detect_model.backbone.fpn.inner_blocks.3.weight torch.Size([256, 2048, 1, 1])\n","detect_model.backbone.fpn.inner_blocks.3.bias torch.Size([256])\n","detect_model.backbone.fpn.layer_blocks.0.weight torch.Size([256, 256, 3, 3])\n","detect_model.backbone.fpn.layer_blocks.0.bias torch.Size([256])\n","detect_model.backbone.fpn.layer_blocks.1.weight torch.Size([256, 256, 3, 3])\n","detect_model.backbone.fpn.layer_blocks.1.bias torch.Size([256])\n","detect_model.backbone.fpn.layer_blocks.2.weight torch.Size([256, 256, 3, 3])\n","detect_model.backbone.fpn.layer_blocks.2.bias torch.Size([256])\n","detect_model.backbone.fpn.layer_blocks.3.weight torch.Size([256, 256, 3, 3])\n","detect_model.backbone.fpn.layer_blocks.3.bias torch.Size([256])\n","detect_model.rpn.head.conv.weight torch.Size([256, 256, 3, 3])\n","detect_model.rpn.head.conv.bias torch.Size([256])\n","detect_model.rpn.head.cls_logits.weight torch.Size([3, 256, 1, 1])\n","detect_model.rpn.head.cls_logits.bias torch.Size([3])\n","detect_model.rpn.head.bbox_pred.weight torch.Size([12, 256, 1, 1])\n","detect_model.rpn.head.bbox_pred.bias torch.Size([12])\n","detect_model.roi_heads.box_head.fc6.weight torch.Size([1024, 12544])\n","detect_model.roi_heads.box_head.fc6.bias torch.Size([1024])\n","detect_model.roi_heads.box_head.fc7.weight torch.Size([1024, 1024])\n","detect_model.roi_heads.box_head.fc7.bias torch.Size([1024])\n","detect_model.roi_heads.box_predictor.cls_score.weight torch.Size([10, 1024])\n","detect_model.roi_heads.box_predictor.cls_score.bias torch.Size([10])\n","detect_model.roi_heads.box_predictor.bbox_pred.weight torch.Size([40, 1024])\n","detect_model.roi_heads.box_predictor.bbox_pred.bias torch.Size([40])\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["112611082"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"PqvinEeeQvnw","executionInfo":{"status":"ok","timestamp":1621240855468,"user_tz":-480,"elapsed":11958,"user":{"displayName":"Yujun Kong","photoUrl":"","userId":"16664495913434421972"}}},"source":["#because pirl is trained with DataParallel, we need to modify the key of the state_dict\n","def preprocess_state_dict(file_path):\n","    state_dict = torch.load(file_path)\n","    from collections import OrderedDict\n","    new_state_dict = OrderedDict()\n","\n","    for k, v in state_dict.items():\n","        if 'module'  in k:\n","            k = k[7:]\n","        new_state_dict[k]=v\n","    torch.save(new_state_dict,file_path)\n","\n","preprocess_state_dict('./model/pirl_jigsaw/rep_net3.pth')"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"17AQuWwtmVVj","executionInfo":{"status":"ok","timestamp":1621240859452,"user_tz":-480,"elapsed":3162,"user":{"displayName":"Yujun Kong","photoUrl":"","userId":"16664495913434421972"}}},"source":["#split train test\n","args = {'bs': 2}\n","\n","labeled_val_scene_index = np.arange(128, 134)\n","labeld_train_scence_index = np.arange(106,128)\n","\n","from helper import collate_fn\n","def get_transform_task1():\n","    transform = torchvision.transforms.Compose([torchvision.transforms.Resize((198,198)),\n","                 torchvision.transforms.ToTensor(),\n","                 torchvision.transforms.Normalize(mean = [0.485, 0.456, 0.406],\n","                                                  std = [0.229, 0.224, 0.225])])\n","    return transform\n","\n","def get_transform_task2():\n","    transform = torchvision.transforms.Compose([torchvision.transforms.Resize((198,198)),\n","                    torchvision.transforms.ToTensor(),\n","                    torchvision.transforms.Normalize(mean = [0.485, 0.456, 0.406],\n","                                                    std = [0.229, 0.224, 0.225])])\n","    return transform \n","trainset_task1 = LabeledDataset(\n","    image_folder=image_folder,\n","    annotation_file=annotation_csv,\n","    scene_index=labeld_train_scence_index,\n","    transform=get_transform_task1(),\n","    extra_info=False\n","    )\n","\n","\n","trainset_task2 = LabeledDataset(\n","    image_folder=image_folder,\n","    annotation_file=annotation_csv,\n","    scene_index=labeld_train_scence_index,\n","    transform=get_transform_task2(),\n","    extra_info=False\n","    )\n","\n","\n","trainloader_task1 = torch.utils.data.DataLoader(\n","    trainset_task1,\n","    batch_size=args['bs'],\n","    shuffle=True,\n","    num_workers=2,\n","    collate_fn=collate_fn\n","    )\n","\n","\n","trainloader_task2 = torch.utils.data.DataLoader(\n","    trainset_task2,\n","    batch_size=args['bs'],\n","    shuffle=True,\n","    num_workers=2,\n","    collate_fn=collate_fn\n","    )\n","\n","\n","# For bounding boxes task\n","labeled_trainset_task1 = LabeledDataset(\n","    image_folder=image_folder,\n","    annotation_file=annotation_csv,\n","    scene_index=labeled_val_scene_index,\n","    transform=get_transform_task1(),\n","    extra_info=False\n","    )\n","dataloader_task1 = torch.utils.data.DataLoader(\n","    labeled_trainset_task1,\n","    batch_size=1,\n","    shuffle=False,\n","    num_workers=2\n","    )\n","# For road map task\n","labeled_trainset_task2 = LabeledDataset(\n","    image_folder=image_folder,\n","    annotation_file=annotation_csv,\n","    scene_index=labeled_val_scene_index,\n","    transform=get_transform_task2(),\n","    extra_info=False\n","    )\n","dataloader_task2 = torch.utils.data.DataLoader(\n","    labeled_trainset_task2,\n","    batch_size=1,\n","    shuffle=False,\n","    num_workers=2\n","    )\n","\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"OZzdsN4JTB5A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621240860837,"user_tz":-480,"elapsed":782,"user":{"displayName":"Yujun Kong","photoUrl":"","userId":"16664495913434421972"}},"outputId":"92a9b7b6-4299-4a2c-b364-58ea4b79b093"},"source":["!ls model"],"execution_count":12,"outputs":[{"output_type":"stream","text":["box_detect\t     pirl_jigsaw  road_seg_BH\troad_seg_SSL_new\n","model_v1_state_dict  road_seg\t  road_seg_SSL\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uRjpo1dIbeoA","colab":{"base_uri":"https://localhost:8080/","height":411},"executionInfo":{"status":"error","timestamp":1621240972313,"user_tz":-480,"elapsed":110735,"user":{"displayName":"Yujun Kong","photoUrl":"","userId":"16664495913434421972"}},"outputId":"740269a4-477f-4e28-f1cf-a74f13e26ce0"},"source":["def train_road_model(road_model, optimizer, criterion,  trainloader, dataloader_task2, epoch, best_TS, log_interval = 50):\n","    road_model.train()\n","    running_loss = 0\n","\n","    for itr, (images, bbox, road_image ) in enumerate(trainloader):      \n","        images = torch.stack(images).to(device)\n","        road_image = torch.stack(road_image).to(device)\n","     \n","        optimizer.zero_grad()\n","            \n","        output = road_model(images)\n","        loss = criterion(output, road_image*1)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss\n","\n","        if itr > 0 and itr % log_interval == 0:\n","            print(\"\"\"Train Epoch: {} [{}/{} ({:.0f}%)]\\t \n","                    Current Loss: {:.6f}, Average Loss: {:.6f}\"\"\".format(\n","                epoch, itr * len(images), len(trainloader.dataset),\n","                100. * itr / len(trainloader), loss.item(), running_loss/log_interval))\n","            running_loss =  0\n","    #val once\n","\n","    road_model.eval()\n","    total = 0\n","    total_ts_road_map = 0\n","    with torch.no_grad():\n","        for i, data in enumerate(dataloader_task2):\n","            total += 1\n","            sample, target, road_image = data\n","            sample = sample.cuda()\n","\n","            predicted_road_map = model_loader.get_binary_road_map(sample).cpu()\n","            ts_road_map = helper.compute_ts_road_map(predicted_road_map, road_image)\n","            total_ts_road_map += ts_road_map\n","\n","    road_map_score = total_ts_road_map / total\n","    print(f'Road Map Score: {road_map_score:.4}')\n","    if epoch >= 1 and road_map_score > best_TS:\n","        best_TS = road_map_score\n","        snapshot_name = 'fintune_epoch_%.0f_raod_map_score_%.5f_lr_%.10f' % (\n","                epoch, road_map_score, optimizer.param_groups[0]['lr']\n","            )\n","        torch.save(road_model.state_dict(), './model/road_seg/' + snapshot_name + '.pth')\n","        torch.save(optimizer.state_dict(), './model/road_seg/opt_' + snapshot_name + '.pth')\n","\n","    return road_map_score, best_TS\n","\n","best_TS = 0.1\n","lr         = 1e-5\n","momentum   = 0\n","w_decay    = 0.01\n","log_interval = 100\n","\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = optim.AdamW(model_loader.road_model.parameters(), lr= lr, betas=(0.9,0.999), eps=1e-08, weight_decay=w_decay, amsgrad=False)\n","#optimizer = optim.RMSprop(model_loader.road_model.parameters(), lr=lr, momentum=momentum, weight_decay=w_decay)\n","scheduler = ReduceLROnPlateau(optimizer,'max', patience= 2, min_lr=1e-10, verbose=True) #use it on TS\n","\n","\n","for epoch in range(30):    \n","    road_map_score, best_TS = train_road_model(model_loader.road_model.to(device), optimizer, criterion, trainloader_task2, dataloader_task2, epoch, best_TS)\n","    scheduler.step(road_map_score)  \n","\n","\n"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Train Epoch: 0 [100/2772 (4%)]\t \n","                    Current Loss: 0.505769, Average Loss: 0.476861\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-4bcbcce5ef78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mroad_map_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_TS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_road_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroad_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader_task2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_task2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_TS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroad_map_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-4bcbcce5ef78>\u001b[0m in \u001b[0;36mtrain_road_model\u001b[0;34m(road_model, optimizer, criterion, trainloader, dataloader_task2, epoch, best_TS, log_interval)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroad_image\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mroad_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroad_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"-1hmLnpmmT_D","colab":{"base_uri":"https://localhost:8080/","height":377},"executionInfo":{"status":"error","timestamp":1621241886694,"user_tz":-480,"elapsed":8994,"user":{"displayName":"Yujun Kong","photoUrl":"","userId":"16664495913434421972"}},"outputId":"aafee8a9-90d9-4791-ebcc-a19c760cd536"},"source":["#-----------------------------------------------------------------------------------------------------------------\n","def train_box_model(box_model, optimizer, trainloader, dataloader_task1, epoch,best_ats, log_interval = 100):\n","    box_model.train()\n","    running_loss = 0\n","\n","    for itr, (images, bbox, road_image ) in enumerate(trainloader): \n","        images = torch.stack(images).to(device)\n","        loss_dict = box_model(images,bbox)\n","        losses = sum(loss for loss in loss_dict.values()) \n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","        #include lr scheduler\n","        running_loss += losses\n","\n","        if itr > 0 and itr % log_interval == 0:\n","            print(\"\"\"Train Epoch: {} [{}/{} ({:.0f}%)]\\t \n","                    Average Loss: {:.6f}\"\"\".format(\n","                epoch, itr * len(images), len(trainloader.dataset),\n","                100. * itr / len(trainloader),  running_loss/log_interval))\n","            running_loss =  0\n","\n","    # #val once\n","    total = 0\n","    total_ats_bounding_boxes = 0\n","    box_model.eval()\n","\n","    with torch.no_grad():\n","        for i, data in enumerate(dataloader_task1):\n","            total += 1\n","            sample, target, road_image = data\n","            sample = sample.cuda()\n","            output = box_model(sample)\n","            pred_boxes = output['boxes'][0].cpu()\n","                #Future work: deal with outputs with 0 objects\n","            if pred_boxes[0].numel() == 0:\n","                pred_boxes = tuple(torch.zeros(len(samples),1,2,4))\n","\n","            ats_bounding_boxes = helper.compute_ats_bounding_boxes(pred_boxes, target['bounding_box'][0])\n","            total_ats_bounding_boxes += ats_bounding_boxes\n","    \n","        avg_ats = total_ats_bounding_boxes/total\n","        # Print loss (uncomment lines below once implemented)\n","        print('\\nValidataion set: Average ATS: {:.4f}\\n'.format(avg_ats))\n","        #save model if they are better\n","        if epoch >= 1 and avg_ats > best_ats:\n","            best_ats = avg_ats\n","            snapshot_name = 'fintune_epoch_%.0f_avg_ats_%.5f_lr_%.10f' % (\n","                    epoch, avg_ats, optimizer.param_groups[0]['lr']\n","                )\n","            torch.save(box_model.state_dict(), './model/box_detect/' + snapshot_name + '.pth')\n","            torch.save(optimizer.state_dict(), './model/box_detect/opt_' + snapshot_name + '.pth')\n","\n","    return avg_ats, best_ats\n","\n","\n","lr         = 1e-5\n","momentum   = 0.9\n","w_decay    = 0.0005\n","best_ats = 0.01862\n","\n","optimizer = torch.optim.SGD(model_loader.box_model.parameters(), lr=lr,\n","                           momentum=momentum, weight_decay=w_decay)\n","optimizer.load_state_dict(torch.load('./model/box_detect/opt_fintune_epoch_2_avg_ats_0.01862_lr_0.0001000000.pth'))\n","for state in optimizer.state.values():\n","    for k, v in state.items():\n","        if isinstance(v, torch.Tensor):\n","            state[k] = v.cuda()\n","\n","scheduler = ReduceLROnPlateau(optimizer,'max', patience=2, min_lr=1e-10, verbose=True) \n","\n","for epoch in range(3, 208):    \n","    avg_ats, best_ats = train_box_model(model_loader.box_model.to(device), optimizer, trainloader_task1, dataloader_task1, epoch, best_ats)\n","    scheduler.step(avg_ats)  \n"],"execution_count":16,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-ba07dbf8701e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m208\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mavg_ats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_ats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_box_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader_task1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_task1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_ats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_ats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-ba07dbf8701e>\u001b[0m in \u001b[0;36mtrain_box_model\u001b[0;34m(box_model, optimizer, trainloader, dataloader_task1, epoch, best_ats, log_interval)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroad_image\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-c1bfd258accb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_down\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#loss_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_down\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#list of dictionary of keys 'boxes', 'labels', 'scores'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# Check for degenerate boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mimage_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mimage_sizes_list\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_sizes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mbatch_images\u001b[0;34m(self, images, size_divisible)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mbatched_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_img\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mpad_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatched_imgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: A view was created in no_grad mode and is being modified inplace with grad mode enabled. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one."]}]}]}